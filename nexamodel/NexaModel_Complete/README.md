# NexaModel - Multimodal Emotion Recognition System

## Overview
NexaModel is a state-of-the-art multimodal emotion recognition system that analyzes facial expressions, voice patterns, and text sentiment to accurately identify human emotions.

## Model Information
- **Version**: 1.0.0
- **Created**: 2025-11-14 21:31:00
- **Architecture**: Multimodal CNN + Dense + LSTM with Attention Fusion
- **Emotions Detected**: angry, disgust, fear, happy, neutral, sad, surprise

## Performance Metrics
- **Multimodal Model Accuracy**: 27.27% (Test Set)
- **Face-only Model**: 12.73%
- **Audio-only Model**: 9.09%
- **Text-only Model**: 24.55%

## Files in this Package
```
NexaModel_Complete/
├── multimodal_emotion_model.keras      # Main multimodal model
├── multimodal_model.weights.h5         # Model weights only
├── face_only_model.keras               # Face emotion model
├── audio_only_model.keras              # Audio emotion model
├── text_only_model.keras               # Text emotion model
├── model_config.json                   # Model configuration
├── face_processor.pkl                  # Face preprocessing pipeline
├── audio_processor.pkl                 # Audio preprocessing pipeline
├── text_processor.pkl                  # Text preprocessing pipeline
├── training_histories.pkl              # Training history data
├── evaluation_results.json             # Model evaluation results
├── nexamodel_deployment.py             # Deployment script
└── README.md                           # This file
```

## Quick Start

### 1. Installation
```python
pip install tensorflow opencv-python librosa scikit-learn numpy
```

### 2. Basic Usage
```python
from nexamodel_deployment import NexaEmotionRecognizer

# Initialize the model
recognizer = NexaEmotionRecognizer(model_dir="./")

# Predict emotion from multiple modalities
result = recognizer.predict_emotion(
    face_image="path/to/face_image.jpg",
    audio_file="path/to/audio.wav",
    text="I'm feeling excited about this project!"
)

print(f"Emotion: {result['predicted_emotion']}")
print(f"Confidence: {result['confidence']:.3f}")
```

## Model Architecture

### Multimodal Architecture
- **Face Branch**: CNN (Convolutional Neural Network)
  - Input: 48x48 grayscale face images
  - Layers: Conv2D + MaxPooling + Dropout + Dense

- **Audio Branch**: Dense Neural Network
  - Input: 56-dimensional audio features (MFCC + Spectral + Chroma)
  - Layers: Dense + Dropout + BatchNormalization

- **Text Branch**: LSTM (Long Short-Term Memory)
  - Input: Text sequences (max length 128)
  - Layers: Embedding + LSTM + Dropout + Dense

- **Fusion Layer**: Attention-based fusion mechanism
  - Combines features from all three modalities
  - Final classification: 7 emotion classes

### Training Data
- **Face Data**: FER2013 dataset (facial expressions)
- **Audio Data**: RAVDESS dataset (emotional speech)
- **Text Data**: Emotions dataset for NLP

## Integration with Nexaa Platform

### API Integration
The model can be integrated with the Nexaa backend using the following approach:

1. **Model Loading**: Load the model in the Spring Boot application
2. **Preprocessing**: Use the included preprocessors for data preparation
3. **Prediction**: Call the model for real-time emotion recognition
4. **Response**: Return structured emotion analysis results

### Example Integration Code
```java
// Java (Spring Boot) - Model Service
@Service
public class EmotionRecognitionService {
    // Integration code for calling the Python model
    // via REST API or direct Python execution
}
```

## Performance Notes
- Best performance is achieved when all three modalities are available
- Text-only model performs reasonably well for sentiment analysis
- Face and audio models complement each other effectively
- Model confidence scores indicate prediction reliability

## Support and Maintenance
- Monitor model performance in production
- Retrain periodically with new data for improved accuracy
- Consider fine-tuning for domain-specific applications

## Version History
- v1.0.0: Initial release with multimodal architecture
  - Face, audio, and text emotion recognition
  - Attention-based fusion mechanism
  - Comprehensive evaluation and testing

---
*Generated by NexaModel Training Pipeline*
